Lecture 2 Notes ~ Tuesday ~ Descriptive Statistics\nIntroduction Data is measurable (can be encoded) and objective (raw data should not have human biases).\n\nIf and how the data of interest can be encoded determines the appropriate tools to extract their information. This is not trivial. Remember last week I said the greatest contribution, in my opinion, of data science has been the methods to represent various types of data. There are things we still don't know how to encode in many fields. In others, we figured out a way of encoding the relevant data but do not have the methods to study it yet. In statistics, we often deal with data that can be encoded using one of the 4 scales of measurement we covered last week (nominal, ordinal, interval, ratio) and very clearly defined methodology depending on the scale used. Objectiveness will depend on the data generation process (if collected from Nature or simulated artificially) or data source (primary or secondary)\nInformation is a quantity generated through measurement of a given system.\nWe need to understand the language or syntax underlying the system (how it should be encoded) of interest to be able to explore it and, ultimately, comprehend it. For this, we take measurements and run experiments to find out what works (but more importantly what does not work)\nFrom physics, all measurement entails an energy cost\nMeasurement reduces our uncertainty or ignorance of a system relative to a unit of measurement.\nNaturally, the importance or relevance of reducing our ignorance of a given subject will determine the level of interest and investment available.\nWe are very interested in understanding the physical and socio-cultural world around us, which is why physics and economics are very influential disciplines. Not so many people are interested in abstract number theory, and so they don't get many benefits besides the intellectual pride of solving math problems.\nStatistical methods guide us on how to perform measurement (data collection and sampling methods) and reduce our ignorance of a subject relative to some context.\nLetâ€™s take a look at a more formal distinction between data, information, and statistics\n\nData, Information, & Statistics\n\nData: Observations collected from the real world. For example, in economics, data can be collected on consumer spending, GDP, or stock prices.\nPrimary Data: Collected directly from participants, e.g., conducting surveys on consumer preferences.\nSecondary Data: Collected from already published sources, e.g., using historical GDP data from government reports.\nInformation: Processed data that is meaningful for decision-making. For instance, turning raw data on consumer spending into insights about spending patterns and trends.\nStatistics: The science of collecting, analyzing, and interpreting data, a fundamental tool in economics.\nDescriptive Statistics: Describe data, helping economists understand the central tendencies (e.g., mean income) and variability (e.g., income inequality) of economic data.\nInferential Statistics: Make inferences about larger populations based on sample data. For example, using a sample of consumers to make predictions about the preferences of the entire population.\nUnivariate Frequency Distributions/Tables\nAsk them to think of a variable of interest (discrete or continuous).\nRecall discrete data are those that can be counted, or grouped into distinct values, while continuous data is defined over a range (where a true zero may or may not exist)\nThen create two lists: 1 for the values and 1 for the frequencies/observations. Now explain tables.\n\nIf continuous write a range\n\nFrequency Distribution: Organizes data into values and their frequencies. This is essential in economics, where data like income levels or unemployment rates are often summarized in frequency tables.\nAbsolute Frequency: The number of times a value occurs in a dataset, such as counting how many times a specific income level appears in a survey of households.\nRelative Frequency: It provides the frequency of a value relative to the dataset size. For instance, in analyzing the distribution of income, you may be interested in the percentage of households falling into specific income brackets.\nDiscrete Type: Values are distinct and separate, like the categories of economic classes (poverty, middle class, upper class, working class).\nStudents should build their frequency table for discrete data with all three columns\nContinuous Type: Values from ranges (bins), typically in economic contexts when dealing with continuous variables like income or price ranges.\nStudents should build their frequency table for continuous data. Note that we must build intervals from the data, how should we do this for continuous data? It will depend on the context of the variable of interest, that is, it depends on the information that our data represents.\nBreaking down continuous data into intervals is called grouping, and it yields a grouped frequency table. It brings out the overall pattern a little bit more clearly. But note that we lose information about the individual values observed. \nFormal Representation\nIn economic research, we often represent data in frequency tables or histograms to gain insights into distributions.\nLet $x$ be the variable, $n(x_i)$ the absolute frequency, and $\frac{n(x_i)}{n}$ the relative frequency. This is used when analyzing data related to income distribution, where $x$ represents income levels, $n(x_i)$ the number of households in each income bracket, and $\frac{n(x_i)}{n}$ the proportion of households in each bracket.\n$sum_i^k n(x_i) = n$ (the sample size)\nNow, once we have a table, our first problem is to sort them and summarize them, so that both we and other people will be able to make sense of our observations.\nMeasures of Centrality\nBy central tendency we mean the tendency of the observations to center around a particular value (or pile up in a particular category) rather than spread themselves evenly across a range or among the available categories.\nThere are three commonly used averages (i.e., moments) or measures of centrality. Which one we use depends on the type of variable and distribution of our data.\nMean (Average): $\bar{x} = \frac{1}{n}sum_{i=1}^{n}x_i$\nIn economics, mean income is a key measure for understanding the economic well-being of a population.\nThe mean has the advantage of being fairly stable across samples. That is, if we take a number of samples from the same population, their means are likely to differ less than their median and mode. This is not always the case though, consider a very skewed distribution. Then, which measure best represents the center of it? Income and earning data is an example of this case.\nMedian: The middle value when sorted. It's essential when analyzing income distributions because it is less affected by extreme values (outliers) and provides insight into the typical income.\nIn addition to situations in which the distribution is skewed or it has some extreme values (outliers), the median must also be used when there is uncertainty about the sizes of some of the values in the distribution\n\nUnder 12, 22, 48, 54, over 65\nMode: The most frequent value. In economics, it can represent the most common income bracket in a population.