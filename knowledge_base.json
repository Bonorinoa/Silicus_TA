[
   {
        "Topic": "Brief history of statistics and basic concepts",
        "Context": "A Brief History of Statistics\nStatistical thinking\nData was the first invention needed for Statistics to emerge.\nData has been of interest to leaders and rulers for centuries.\nPharaohs, for example, were very interested in collecting data about population, physical traits, and even wealth to compute taxes.\nData has only appreciated with time. Replacing many commodities in the priorities of firms and governments.\nAnother way to think of data is a collection of experiences, and statistics as the collection of methods to help us make sense (summarize, generalize, and predict) of these experiences.\nAlmost by instinct, we start looking for connections and patterns, similarities and differences, among the things we happen to have noticed.\nWe do this mental computation all the time to make decisions and predict potential outcomes.\n“On average, I run about 2 miles every day” (summarizing experience)\n“We can expect a lot of rain at this time of year” (generalizing experience)\n“The earlier you start preparing for the exam, the better you’ll do” (generalizing)\nEugenics and Early Statistics:\nStart by introducing Francis Galton, Charles Darwin's half-cousin, and his connection to the birth of modern statistics.\nExplain that Galton was fascinated by Darwin's theory of evolution and its implications for human society.\nDiscuss how Galton's fascination led to the creation of eugenics (from the greek “eugenes”, which means good in birth), a field that aimed to improve humanity through selective breeding.\nStarted from the assumption that most diseases were inheritable,\nTheir goal was to create a hereditary model to guide policy design for increasing the occurrence of “desirable” traits.\nNote the word occurrence. In this class we will study, for the most part, frequentist statistics. An approach that focuses on frequencies and data for making inferences about reality.\nMention that Galton's work introduced statistical concepts like standard deviation, correlation, and regression analysis.\nThrough his work in biometrical genetics.\nAddress the ethical implications of Galton's work, particularly its misuse by the Nazis, and highlight the importance of responsible and ethical statistical use.\nThe nazi experiment was an application of Eugenics. Many of the methods developed by Galton and his followers played a big role in validating Nazi’s beliefs and justifying birth control policies such as euthanasia.\nIt was all according to the “data”, there is nothing more objective than raw data, right? After all, data is just a collection of observations. Do you guys think data can be subjective or be influenced by other factors? Does how it is collected matter?\nRonald Fisher, Karl Pearson, and the Frequentist School\nMove on to discuss the contributions of Ronald Fisher and Karl Pearson to statistics.\nExplain Fisher's emphasis on experimental design, hypothesis testing, and the introduction of the p-value (and the p=0.05 threshold).\nDescribe how correlation was used as a measure of causation and point out its limitations.\nFisher and Pearson are credited with establishing the statistician as an unbiased authority and correlation as the sufficient condition for causation.\nAs we will see, a correlation is a measure that helps us quantify the overall level of association between two variables. The problem is that it will tell you how two variables vary together (i.e., covary) but not what is causing them to vary in the first place. We’ll come back to this topic in a few slides.\nCritics of these views about correlation argued that other factors, such as confounding variables or chance, could be responsible for the observed correlations. This debate led to the emergence of alternative approaches…\nEmergence of Bayesian School of Thought\nIntroduce the Bayesian school of thought as an alternative approach to statistics.\nWhile frequentist statistics relies on long-run frequencies and probability distributions derived purely from observed data, the Bayesian approach brings prior beliefs and knowledge into the equation.\nExplain that Bayesian probability incorporates prior beliefs and knowledge into analysis.\nBayesian probability considers not only the data at hand but also integrates existing information, beliefs, and expertise. This is achieved through the concept of prior probabilities, where we express our initial beliefs about an event's likelihood before observing any data. As new evidence accumulates, the Bayesian approach allows us to update our beliefs using Bayes' theorem, resulting in a posterior probability distribution.\nGive an example of Bayesian reasoning, such as updating probabilities based on new evidence (e.g., weather forecasting).\nMention that Bayesian methods were initially challenging due to computational limitations, but they're now widely used for complex analysis.\nCalculating posterior distributions often required complex mathematical computations that were not feasible without modern computing power.\nSome of these methods (Bayesian networks, Markov chains, Monte carlo simulations) are now widely used for complex analyses across various fields such as machine learning, data science, finance, and more.\nSewall Wright and Path Diagrams: Unraveling Causality.\nMove on to Sewall Wright's role in understanding causality.\nAmerican geneticist and statistician from the ‘20s who recognized that real-world scenarios often involve intricate causal relationships that go beyond simplistic linear cause-and-effect models.\nExplain that path diagrams were a visual tool for representing complex causal relationships.\nHe introduced path diagrams, a visual tool to represent and visualize complex causal interactions in a structured manner.\nUse a simple example to illustrate a path diagram and its components.\nImagine we're exploring factors influencing a student's academic performance. We have variables like sleep, study hours, stress, and grades. Path diagrams break down the relationships between these variables into paths, representing direct and indirect effects.\nIn our example, a path diagram might show how sleep directly influences study hours, how stress indirectly affects grades through its impact on study hours, and how study hours directly affect grades. Each arrow in the diagram signifies a causal relationship.\nHighlight how path diagrams moved researchers beyond simplistic correlations to a more nuanced understanding of cause and effect.\nPath diagram enabled us to move from asking 'what is related to what?' to understanding"

   },
   {
        "Topic": "Descriptive Statistics and the relationships between data, information, and statistics",
        "Context":"Lecture 2 Notes ~ Tuesday ~ Descriptive Statistics\nIntroduction Data is measurable (can be encoded) and objective (raw data should not have human biases).\n\nIf and how the data of interest can be encoded determines the appropriate tools to extract their information. This is not trivial. Remember last week I said the greatest contribution, in my opinion, of data science has been the methods to represent various types of data. There are things we still don't know how to encode in many fields. In others, we figured out a way of encoding the relevant data but do not have the methods to study it yet. In statistics, we often deal with data that can be encoded using one of the 4 scales of measurement we covered last week (nominal, ordinal, interval, ratio) and very clearly defined methodology depending on the scale used. Objectiveness will depend on the data generation process (if collected from Nature or simulated artificially) or data source (primary or secondary)\nInformation is a quantity generated through measurement of a given system.\nWe need to understand the language or syntax underlying the system (how it should be encoded) of interest to be able to explore it and, ultimately, comprehend it. For this, we take measurements and run experiments to find out what works (but more importantly what does not work)\nFrom physics, all measurement entails an energy cost\nMeasurement reduces our uncertainty or ignorance of a system relative to a unit of measurement.\nNaturally, the importance or relevance of reducing our ignorance of a given subject will determine the level of interest and investment available.\nWe are very interested in understanding the physical and socio-cultural world around us, which is why physics and economics are very influential disciplines. Not so many people are interested in abstract number theory, and so they don't get many benefits besides the intellectual pride of solving math problems.\nStatistical methods guide us on how to perform measurement (data collection and sampling methods) and reduce our ignorance of a subject relative to some context.\nLet’s take a look at a more formal distinction between data, information, and statistics\n\nData, Information, & Statistics\n\nData: Observations collected from the real world. For example, in economics, data can be collected on consumer spending, GDP, or stock prices.\nPrimary Data: Collected directly from participants, e.g., conducting surveys on consumer preferences.\nSecondary Data: Collected from already published sources, e.g., using historical GDP data from government reports.\nInformation: Processed data that is meaningful for decision-making. For instance, turning raw data on consumer spending into insights about spending patterns and trends.\nStatistics: The science of collecting, analyzing, and interpreting data, a fundamental tool in economics.\nDescriptive Statistics: Describe data, helping economists understand the central tendencies (e.g., mean income) and variability (e.g., income inequality) of economic data.\nInferential Statistics: Make inferences about larger populations based on sample data. For example, using a sample of consumers to make predictions about the preferences of the entire population.\nUnivariate Frequency Distributions/Tables\nAsk them to think of a variable of interest (discrete or continuous).\nRecall discrete data are those that can be counted, or grouped into distinct values, while continuous data is defined over a range (where a true zero may or may not exist)\nThen create two lists: 1 for the values and 1 for the frequencies/observations. Now explain tables.\n\nIf continuous write a range\n\nFrequency Distribution: Organizes data into values and their frequencies. This is essential in economics, where data like income levels or unemployment rates are often summarized in frequency tables.\nAbsolute Frequency: The number of times a value occurs in a dataset, such as counting how many times a specific income level appears in a survey of households.\nRelative Frequency: It provides the frequency of a value relative to the dataset size. For instance, in analyzing the distribution of income, you may be interested in the percentage of households falling into specific income brackets.\nDiscrete Type: Values are distinct and separate, like the categories of economic classes (poverty, middle class, upper class, working class).\nStudents should build their frequency table for discrete data with all three columns\nContinuous Type: Values from ranges (bins), typically in economic contexts when dealing with continuous variables like income or price ranges.\nStudents should build their frequency table for continuous data. Note that we must build intervals from the data, how should we do this for continuous data? It will depend on the context of the variable of interest, that is, it depends on the information that our data represents.\nBreaking down continuous data into intervals is called grouping, and it yields a grouped frequency table. It brings out the overall pattern a little bit more clearly. But note that we lose information about the individual values observed. \nFormal Representation\nIn economic research, we often represent data in frequency tables or histograms to gain insights into distributions.\nLet $x$ be the variable, $n(x_i)$ the absolute frequency, and $\frac{n(x_i)}{n}$ the relative frequency. This is used when analyzing data related to income distribution, where $x$ represents income levels, $n(x_i)$ the number of households in each income bracket, and $\frac{n(x_i)}{n}$ the proportion of households in each bracket.\n$sum_i^k n(x_i) = n$ (the sample size)\nNow, once we have a table, our first problem is to sort them and summarize them, so that both we and other people will be able to make sense of our observations.\nMeasures of Centrality\nBy central tendency we mean the tendency of the observations to center around a particular value (or pile up in a particular category) rather than spread themselves evenly across a range or among the available categories.\nThere are three commonly used averages (i.e., moments) or measures of centrality. Which one we use depends on the type of variable and distribution of our data.\nMean (Average): $\bar{x} = \frac{1}{n}sum_{i=1}^{n}x_i$\nIn economics, mean income is a key measure for understanding the economic well-being of a population.\nThe mean has the advantage of being fairly stable across samples. That is, if we take a number of samples from the same population, their means are likely to differ less than their median and mode. This is not always the case though, consider a very skewed distribution. Then, which measure best represents the center of it? Income and earning data is an example of this case.\nMedian: The middle value when sorted. It's essential when analyzing income distributions because it is less affected by extreme values (outliers) and provides insight into the typical income.\nIn addition to situations in which the distribution is skewed or it has some extreme values (outliers), the median must also be used when there is uncertainty about the sizes of some of the values in the distribution\n\nUnder 12, 22, 48, 54, over 65\nMode: The most frequent value. In economics, it can represent the most common income bracket in a population."
   }
]